{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz_Bc1esGrIO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('BONN epilepsy dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/BONN_epilepsy_dataset')\n",
        "parent_dir = '/content/BONN_epilepsy_dataset'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "parent_dir = '/content/BONN_epilepsy_dataset/BONN epilepsy dataset'\n",
        "print(\"Contents of the updated dataset directory:\")\n",
        "print(os.listdir(parent_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awhqWcTaWl5e",
        "outputId": "94a707d5-d80d-4870-9211-17368e3da71c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of the updated dataset directory:\n",
            "['F', 'O', 'S']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "parent_dir = '/content/BONN_epilepsy_dataset/BONN epilepsy dataset'\n",
        "directories = ['S', 'F', 'O']\n",
        "labels_map = {'S': 'boundary', 'F': 'unknown', 'O': 'safe'}  #label mapping: \"safe\" and \"boundary\"\n",
        "\n",
        "def load_data(parent_dir, directories):\n",
        "    eeg_data = []\n",
        "    labels = []\n",
        "\n",
        "    for dir_name in directories:\n",
        "        dir_path = os.path.join(parent_dir, dir_name)\n",
        "        for file_name in sorted(os.listdir(dir_path)):\n",
        "            file_path = os.path.join(dir_path, file_name)\n",
        "            with open(file_path, 'r') as file:\n",
        "                data_points = [int(line.strip()) for line in file]\n",
        "                eeg_data.append(data_points)\n",
        "                labels.append(labels_map[dir_name])\n",
        "\n",
        "    return np.array(eeg_data), np.array(labels)\n",
        "\n",
        "eeg_data, labels = load_data(parent_dir, directories)\n",
        "\n",
        "def extract_features(eeg_data):\n",
        "    \"\"\"\n",
        "    Extracts statistical features from EEG data.\n",
        "    This includes variance, kurtosis, and entropy.\n",
        "    \"\"\"\n",
        "    eeg_data_safe = np.where(eeg_data > 0, eeg_data, 1e-9)\n",
        "\n",
        "    variance = np.var(eeg_data, axis=1)\n",
        "    kurtosis = np.mean((eeg_data - np.mean(eeg_data, axis=1, keepdims=True))**4, axis=1)\n",
        "    entropy = -np.sum(eeg_data_safe * np.log2(eeg_data_safe), axis=1)\n",
        "    features = np.vstack([variance, kurtosis, entropy]).T\n",
        "    return features\n",
        "\n",
        "#kNN boundary detection for F samples\n",
        "def kNN_boundary_detection(f_data, knn_model):\n",
        "    \"\"\"\n",
        "    Classifies F samples into 'safe', 'boundary', or 'noise' based on kNN model trained on O and S.\n",
        "    \"\"\"\n",
        "    features = extract_features(f_data)\n",
        "    predictions = knn_model.predict(features)\n",
        "    return predictions\n",
        "\n",
        "#training data (only O and S samples)\n",
        "o_indices = labels == 'safe'\n",
        "s_indices = labels == 'boundary'\n",
        "\n",
        "#we combined O and S samples\n",
        "train_data = np.vstack([eeg_data[o_indices], eeg_data[s_indices]])\n",
        "train_labels = np.concatenate([labels[o_indices], labels[s_indices]])\n",
        "\n",
        "train_features = extract_features(train_data)\n",
        "\n",
        "#trainning the kNN classifier on O and S samples\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(train_features, train_labels)\n",
        "\n",
        "#F samples classification\n",
        "f_indices = labels == 'unknown'\n",
        "f_data = eeg_data[f_indices]\n",
        "eeg_types_f = kNN_boundary_detection(f_data, knn)\n",
        "\n",
        "print(\"Classifications for F (epileptic non-ictal) samples:\", eeg_types_f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rEujZg2Wn_Z",
        "outputId": "7f7c3849-2038-41ee-b879-3661a097011d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifications for F (epileptic non-ictal) samples: ['safe' 'boundary' 'safe' 'safe' 'safe' 'safe' 'safe' 'boundary'\n",
            " 'boundary' 'boundary' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'safe' 'safe' 'boundary' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'boundary' 'boundary' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'safe' 'safe' 'safe' 'boundary' 'safe' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'safe' 'safe' 'boundary' 'boundary' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'safe' 'safe' 'safe' 'boundary' 'safe' 'safe' 'boundary' 'safe'\n",
            " 'safe' 'boundary' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'boundary' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'safe' 'safe' 'boundary' 'safe' 'safe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "latent_dim = 100\n",
        "data_dim = eeg_data.shape[1]\n",
        "num_classes = 3  # 0 = safe, 1 = boundary, 2 = noise\n",
        "batch_size = 64\n",
        "epochs = 500\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "class ConditionalGenerator(nn.Module):\n",
        "    def __init__(self, latent_dim, data_dim, num_classes):\n",
        "        super(ConditionalGenerator, self).__init__()\n",
        "        self.label_embedding = nn.Embedding(num_classes, num_classes)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim + num_classes, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, data_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, labels):\n",
        "        label_embeddings = self.label_embedding(labels)\n",
        "        input = torch.cat((z, label_embeddings), dim=1)\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class ConditionalDiscriminator(nn.Module):\n",
        "    def __init__(self, data_dim, num_classes):\n",
        "        super(ConditionalDiscriminator, self).__init__()\n",
        "        self.label_embedding = nn.Embedding(num_classes, num_classes)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(data_dim + num_classes, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        label_embeddings = self.label_embedding(labels)\n",
        "        input = torch.cat((x, label_embeddings), dim=1)\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "generator = ConditionalGenerator(latent_dim, data_dim, num_classes).to(device)\n",
        "discriminator = ConditionalDiscriminator(data_dim, num_classes).to(device)\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "criterion = nn.BCELoss()\n",
        "eeg_data_normalized = (eeg_data - np.min(eeg_data)) / (np.max(eeg_data) - np.min(eeg_data))\n",
        "eeg_data_normalized = 2 * eeg_data_normalized - 1\n",
        "\n",
        "\n",
        "def classify_data(eeg_data, labels, k=5):\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "    neighbors = NearestNeighbors(n_neighbors=k).fit(eeg_data)\n",
        "    types = []\n",
        "    distances, indices = neighbors.kneighbors(eeg_data)\n",
        "\n",
        "    for i, dist in enumerate(indices):\n",
        "        l = sum(labels[neighbor] == 0 for neighbor in dist)  #we count non-ictal (safe) samples\n",
        "        if l > 0.8 * k:\n",
        "            types.append(0)  #safe\n",
        "        elif 0 < l <= 0.5 * k:\n",
        "            types.append(1)  #boundary\n",
        "        else:\n",
        "            types.append(2)  #noise\n",
        "    return np.array(types)\n",
        "\n",
        "real_labels = classify_data(eeg_data_normalized, labels)\n",
        "eeg_tensor = torch.tensor(eeg_data_normalized, dtype=torch.float32).to(device)\n",
        "label_tensor = torch.tensor(real_labels, dtype=torch.long).to(device)\n",
        "dataloader = DataLoader(TensorDataset(eeg_tensor, label_tensor), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for real_samples, real_class_labels in dataloader:\n",
        "        real_samples = real_samples.to(device)\n",
        "        real_class_labels = real_class_labels.to(device)\n",
        "        batch_size_curr = real_samples.size(0)\n",
        "        z = torch.randn(batch_size_curr, latent_dim).to(device)\n",
        "        fake_class_labels = torch.randint(0, num_classes, (batch_size_curr,), dtype=torch.long).to(device)\n",
        "        fake_samples = generator(z, fake_class_labels).detach()\n",
        "        real_targets = torch.ones(batch_size_curr, 1).to(device)\n",
        "        fake_targets = torch.zeros(batch_size_curr, 1).to(device)\n",
        "        optimizer_D.zero_grad()\n",
        "        real_loss = criterion(discriminator(real_samples, real_class_labels), real_targets)\n",
        "        fake_loss = criterion(discriminator(fake_samples, fake_class_labels), fake_targets)\n",
        "        d_loss = real_loss + fake_loss\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "        z = torch.randn(batch_size_curr, latent_dim).to(device)\n",
        "        target_class_labels = torch.randint(0, num_classes, (batch_size_curr,), dtype=torch.long).to(device)\n",
        "        fake_samples = generator(z, target_class_labels)\n",
        "        optimizer_G.zero_grad()\n",
        "        g_loss = criterion(discriminator(fake_samples, target_class_labels), real_targets)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "for label in range(num_classes):\n",
        "    z = torch.randn(1000, latent_dim).to(device)\n",
        "    labels = torch.full((1000,), label, dtype=torch.long).to(device)\n",
        "    synthetic_data = generator(z, labels).detach().cpu().numpy()\n",
        "    print(f\"Synthetic data for class {label}:\")\n",
        "    print(synthetic_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6txnbUMhWqFk",
        "outputId": "3a35bfc4-c22d-4df5-d063-24a58882fc8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/500] | D Loss: 0.8072 | G Loss: 3.9067\n",
            "Epoch [100/500] | D Loss: 0.5875 | G Loss: 2.8797\n",
            "Epoch [150/500] | D Loss: 0.4286 | G Loss: 3.8682\n",
            "Epoch [200/500] | D Loss: 0.3984 | G Loss: 6.6218\n",
            "Epoch [250/500] | D Loss: 0.6660 | G Loss: 3.9887\n",
            "Epoch [300/500] | D Loss: 0.5915 | G Loss: 7.7392\n",
            "Epoch [350/500] | D Loss: 0.5246 | G Loss: 4.1647\n",
            "Epoch [400/500] | D Loss: 0.5689 | G Loss: 4.4039\n",
            "Epoch [450/500] | D Loss: 0.5956 | G Loss: 3.9689\n",
            "Epoch [500/500] | D Loss: 0.4361 | G Loss: 6.7721\n",
            "Synthetic data for class 0:\n",
            "[[-0.15966456 -0.06263401 -0.14382316 ... -0.15718915 -0.14907733\n",
            "  -0.09947086]\n",
            " [-0.17773975 -0.07334375 -0.15298118 ... -0.17437743 -0.1755515\n",
            "  -0.12316857]\n",
            " [-0.14975886 -0.05308864 -0.1451403  ... -0.144986   -0.14598134\n",
            "  -0.08725686]\n",
            " ...\n",
            " [-0.13533655 -0.0512364  -0.12283304 ... -0.13369037 -0.11592359\n",
            "  -0.06177285]\n",
            " [-0.14664648 -0.03151742 -0.12042892 ... -0.15803954 -0.15944616\n",
            "  -0.09026279]\n",
            " [-0.10601423 -0.02527048 -0.09523885 ... -0.11634228 -0.08666462\n",
            "  -0.06623557]]\n",
            "Synthetic data for class 1:\n",
            "[[-0.10801755 -0.0165011  -0.08711549 ... -0.11291283 -0.0764375\n",
            "  -0.0589942 ]\n",
            " [-0.09144021 -0.00153137 -0.09212274 ... -0.09829351 -0.05576251\n",
            "  -0.0471564 ]\n",
            " [-0.1189367  -0.02619565 -0.10863191 ... -0.11821648 -0.09794029\n",
            "  -0.07052208]\n",
            " ...\n",
            " [-0.08453835 -0.01405783 -0.08259334 ... -0.08498333 -0.05625868\n",
            "  -0.0461416 ]\n",
            " [-0.13529111 -0.01741027 -0.11514403 ... -0.15551387 -0.13979565\n",
            "  -0.0784985 ]\n",
            " [-0.0939208  -0.00945059 -0.08559541 ... -0.09761745 -0.06697733\n",
            "  -0.06477595]]\n",
            "Synthetic data for class 2:\n",
            "[[-0.02743143 -0.00024777 -0.03909838 ... -0.04766672 -0.00686643\n",
            "  -0.0349425 ]\n",
            " [-0.0344314   0.0019066  -0.04307272 ... -0.05313804 -0.01355935\n",
            "  -0.03711979]\n",
            " [-0.03644935 -0.00166254 -0.04399113 ... -0.05402236 -0.01422859\n",
            "  -0.03784192]\n",
            " ...\n",
            " [-0.02992845 -0.00310756 -0.04164684 ... -0.05852704 -0.01599771\n",
            "  -0.0369454 ]\n",
            " [-0.0326906  -0.00428347 -0.04170081 ... -0.0519642  -0.01004769\n",
            "  -0.0352169 ]\n",
            " [-0.03660069 -0.00198042 -0.04184858 ... -0.0674656  -0.01138757\n",
            "  -0.03627434]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "parent_dir = '/content/BONN_epilepsy_dataset/BONN epilepsy dataset'\n",
        "\n",
        "def load_bonn_data(parent_dir):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for category_dir in os.listdir(parent_dir):\n",
        "        category_path = os.path.join(parent_dir, category_dir)\n",
        "        if os.path.isdir(category_path):\n",
        "            label = len(labels)\n",
        "            for file in os.listdir(category_path):\n",
        "                file_path = os.path.join(category_path, file)\n",
        "                sample = np.loadtxt(file_path)\n",
        "                data.append(sample)\n",
        "                labels.append(label)\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "real_data, real_labels = load_bonn_data(parent_dir)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(real_data, real_labels, test_size=0.2, random_state=42)\n",
        "k = 10  #number of neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "print(\"Evaluation on Real Data:\")\n",
        "y_pred_real = knn.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_real))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_real))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcwQXUqAbNKq",
        "outputId": "9378f891-18f0-4498-fd35-30674c5c61d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Real Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      1.00      0.55        22\n",
            "         100       0.00      0.00      0.00        16\n",
            "         200       1.00      0.09      0.17        22\n",
            "\n",
            "    accuracy                           0.40        60\n",
            "   macro avg       0.46      0.36      0.24        60\n",
            "weighted avg       0.51      0.40      0.26        60\n",
            "\n",
            "Confusion Matrix:\n",
            " [[22  0  0]\n",
            " [16  0  0]\n",
            " [20  0  2]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def generate_synthetic_data(class_label, num_samples):\n",
        "    \"\"\"\n",
        "    Generate synthetic data using the GAN.\n",
        "    Args:\n",
        "        class_label (int): The class for which data is to be generated.\n",
        "        num_samples (int): Number of samples to generate.\n",
        "    Returns:\n",
        "        numpy.ndarray: Synthetic data of shape (num_samples, num_features).\n",
        "    \"\"\"\n",
        "    generator.eval()\n",
        "    z = torch.randn(num_samples, latent_dim).to(device)\n",
        "    labels = torch.full((num_samples,), class_label, dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "        synthetic_data = generator(z, labels).detach().cpu().numpy()\n",
        "\n",
        "    return synthetic_data\n",
        "\n",
        "num_synthetic_samples = 200\n",
        "synthetic_data = []\n",
        "synthetic_labels = []\n",
        "\n",
        "#we generate synthetic data for each class\n",
        "for class_label in [0, 1, 2]:  #classes: 0 = safe 1 = boundary 2 = noise\n",
        "    data = generate_synthetic_data(class_label, num_synthetic_samples)\n",
        "    synthetic_data.append(data)\n",
        "    synthetic_labels.append(np.full(num_synthetic_samples, class_label))\n",
        "synthetic_data = np.vstack(synthetic_data)\n",
        "synthetic_labels = np.concatenate(synthetic_labels)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_synthetic = knn.predict(synthetic_data)\n",
        "print(\"Evaluation on Synthetic Data:\")\n",
        "print(classification_report(synthetic_labels, y_pred_synthetic))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(synthetic_labels, y_pred_synthetic))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiREM4nacEi0",
        "outputId": "af46b8e5-64df-4693-961a-eb2c92b3c9e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Synthetic Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      1.00      0.50       200\n",
            "           1       0.00      0.00      0.00       200\n",
            "           2       0.00      0.00      0.00       200\n",
            "\n",
            "    accuracy                           0.33       600\n",
            "   macro avg       0.11      0.33      0.17       600\n",
            "weighted avg       0.11      0.33      0.17       600\n",
            "\n",
            "Confusion Matrix:\n",
            " [[200   0   0]\n",
            " [200   0   0]\n",
            " [200   0   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}