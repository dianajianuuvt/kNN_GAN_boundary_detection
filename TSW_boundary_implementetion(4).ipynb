{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jz_Bc1esGrIO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('BONN epilepsy dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/BONN_epilepsy_dataset')\n",
        "parent_dir = '/content/BONN_epilepsy_dataset'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parent_dir = '/content/BONN_epilepsy_dataset/BONN epilepsy dataset'\n",
        "print(\"Contents of the updated dataset directory:\")\n",
        "print(os.listdir(parent_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awhqWcTaWl5e",
        "outputId": "c2581cf4-9737-4187-fb1d-efd484ec61ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of the updated dataset directory:\n",
            "['O', 'F', 'S']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "parent_dir = '/content/BONN_epilepsy_dataset/BONN epilepsy dataset'\n",
        "directories = ['S', 'F', 'O']\n",
        "labels_map = {'S': 'boundary', 'F': 'unknown', 'O': 'safe'}  #label mapping: \"safe\" and \"boundary\"\n",
        "\n",
        "def load_data(parent_dir, directories):\n",
        "    eeg_data = []\n",
        "    labels = []\n",
        "\n",
        "    for dir_name in directories:\n",
        "        dir_path = os.path.join(parent_dir, dir_name)\n",
        "        for file_name in sorted(os.listdir(dir_path)):\n",
        "            file_path = os.path.join(dir_path, file_name)\n",
        "            with open(file_path, 'r') as file:\n",
        "                data_points = [int(line.strip()) for line in file]\n",
        "                eeg_data.append(data_points)\n",
        "                labels.append(labels_map[dir_name])\n",
        "\n",
        "    return np.array(eeg_data), np.array(labels)\n",
        "\n",
        "eeg_data, labels = load_data(parent_dir, directories)\n",
        "\n",
        "def extract_features(eeg_data):\n",
        "    \"\"\"\n",
        "    Extracts statistical features from EEG data.\n",
        "    This includes variance, kurtosis, and entropy.\n",
        "    \"\"\"\n",
        "    eeg_data_safe = np.where(eeg_data > 0, eeg_data, 1e-9)\n",
        "\n",
        "    variance = np.var(eeg_data, axis=1)\n",
        "    kurtosis = np.mean((eeg_data - np.mean(eeg_data, axis=1, keepdims=True))**4, axis=1)\n",
        "    entropy = -np.sum(eeg_data_safe * np.log2(eeg_data_safe), axis=1)\n",
        "    features = np.vstack([variance, kurtosis, entropy]).T\n",
        "    return features\n",
        "\n",
        "#kNN boundary detection for F samples\n",
        "def kNN_boundary_detection(f_data, knn_model):\n",
        "    \"\"\"\n",
        "    Classifies F samples into 'safe', 'boundary', or 'noise' based on kNN model trained on O and S.\n",
        "    \"\"\"\n",
        "    features = extract_features(f_data)\n",
        "    predictions = knn_model.predict(features)\n",
        "    return predictions\n",
        "\n",
        "#training data (only O and S samples)\n",
        "o_indices = labels == 'safe'\n",
        "s_indices = labels == 'boundary'\n",
        "\n",
        "#we combined O and S samples\n",
        "train_data = np.vstack([eeg_data[o_indices], eeg_data[s_indices]])\n",
        "train_labels = np.concatenate([labels[o_indices], labels[s_indices]])\n",
        "\n",
        "train_features = extract_features(train_data)\n",
        "\n",
        "#trainning the kNN classifier on O and S samples\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(train_features, train_labels)\n",
        "\n",
        "#F samples classification\n",
        "f_indices = labels == 'unknown'\n",
        "f_data = eeg_data[f_indices]\n",
        "eeg_types_f = kNN_boundary_detection(f_data, knn)\n",
        "\n",
        "print(\"Classifications for F (epileptic non-ictal) samples:\", eeg_types_f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rEujZg2Wn_Z",
        "outputId": "5aa4ffcf-d0b1-4d70-9d43-643d46f8e48f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifications for F (epileptic non-ictal) samples: ['safe' 'boundary' 'safe' 'safe' 'safe' 'safe' 'safe' 'boundary'\n",
            " 'boundary' 'boundary' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'safe' 'safe' 'boundary' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'boundary' 'boundary' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'safe' 'safe' 'safe' 'boundary' 'safe' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'safe' 'safe' 'boundary' 'boundary' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'safe' 'safe' 'safe' 'boundary' 'safe' 'safe' 'boundary' 'safe'\n",
            " 'safe' 'boundary' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'boundary' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe'\n",
            " 'safe' 'safe' 'safe' 'boundary' 'safe' 'safe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "latent_dim = 100\n",
        "data_dim = eeg_data.shape[1]\n",
        "num_classes = 3  # 0 = safe, 1 = boundary, 2 = noise\n",
        "batch_size = 64\n",
        "epochs = 500\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "class ConditionalGenerator(nn.Module):\n",
        "    def __init__(self, latent_dim, data_dim, num_classes):\n",
        "        super(ConditionalGenerator, self).__init__()\n",
        "        self.label_embedding = nn.Embedding(num_classes, num_classes)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim + num_classes, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, data_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, labels):\n",
        "        label_embeddings = self.label_embedding(labels)\n",
        "        input = torch.cat((z, label_embeddings), dim=1)\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class ConditionalDiscriminator(nn.Module):\n",
        "    def __init__(self, data_dim, num_classes):\n",
        "        super(ConditionalDiscriminator, self).__init__()\n",
        "        self.label_embedding = nn.Embedding(num_classes, num_classes)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(data_dim + num_classes, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        label_embeddings = self.label_embedding(labels)\n",
        "        input = torch.cat((x, label_embeddings), dim=1)\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "generator = ConditionalGenerator(latent_dim, data_dim, num_classes).to(device)\n",
        "discriminator = ConditionalDiscriminator(data_dim, num_classes).to(device)\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "criterion = nn.BCELoss()\n",
        "eeg_data_normalized = (eeg_data - np.min(eeg_data)) / (np.max(eeg_data) - np.min(eeg_data))\n",
        "eeg_data_normalized = 2 * eeg_data_normalized - 1\n",
        "\n",
        "\n",
        "def classify_data(eeg_data, labels, k=5):\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "    neighbors = NearestNeighbors(n_neighbors=k).fit(eeg_data)\n",
        "    types = []\n",
        "    distances, indices = neighbors.kneighbors(eeg_data)\n",
        "\n",
        "    for i, dist in enumerate(indices):\n",
        "        l = sum(labels[neighbor] == 0 for neighbor in dist)  #we count non-ictal (safe) samples\n",
        "        if k >= l > 0.5 * k:\n",
        "            types.append(0)  #safe\n",
        "        elif 0 < l <= 0.5 * k:\n",
        "            types.append(1)  #boundary\n",
        "        else:\n",
        "            types.append(2)  #noise\n",
        "    return np.array(types)\n",
        "\n",
        "real_labels = classify_data(eeg_data_normalized, labels)\n",
        "eeg_tensor = torch.tensor(eeg_data_normalized, dtype=torch.float32).to(device)\n",
        "label_tensor = torch.tensor(real_labels, dtype=torch.long).to(device)\n",
        "dataloader = DataLoader(TensorDataset(eeg_tensor, label_tensor), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for real_samples, real_class_labels in dataloader:\n",
        "        real_samples = real_samples.to(device)\n",
        "        real_class_labels = real_class_labels.to(device)\n",
        "        batch_size_curr = real_samples.size(0)\n",
        "        z = torch.randn(batch_size_curr, latent_dim).to(device)\n",
        "        fake_class_labels = torch.randint(0, num_classes, (batch_size_curr,), dtype=torch.long).to(device)\n",
        "        fake_samples = generator(z, fake_class_labels).detach()\n",
        "        real_targets = torch.ones(batch_size_curr, 1).to(device)\n",
        "        fake_targets = torch.zeros(batch_size_curr, 1).to(device)\n",
        "        optimizer_D.zero_grad()\n",
        "        real_loss = criterion(discriminator(real_samples, real_class_labels), real_targets)\n",
        "        fake_loss = criterion(discriminator(fake_samples, fake_class_labels), fake_targets)\n",
        "        d_loss = real_loss + fake_loss\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "        z = torch.randn(batch_size_curr, latent_dim).to(device)\n",
        "        target_class_labels = torch.randint(0, num_classes, (batch_size_curr,), dtype=torch.long).to(device)\n",
        "        fake_samples = generator(z, target_class_labels)\n",
        "        optimizer_G.zero_grad()\n",
        "        g_loss = criterion(discriminator(fake_samples, target_class_labels), real_targets)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "for label in range(num_classes):\n",
        "    z = torch.randn(1000, latent_dim).to(device)\n",
        "    labels = torch.full((1000,), label, dtype=torch.long).to(device)\n",
        "    synthetic_data = generator(z, labels).detach().cpu().numpy()\n",
        "    print(f\"Synthetic data for class {label}:\")\n",
        "    print(synthetic_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6txnbUMhWqFk",
        "outputId": "1c18253d-a3c4-4e7b-9674-1397c5d5b878"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/500] | D Loss: 0.6471 | G Loss: 2.7824\n",
            "Epoch [100/500] | D Loss: 0.5809 | G Loss: 3.2908\n",
            "Epoch [150/500] | D Loss: 0.5234 | G Loss: 5.7301\n",
            "Epoch [200/500] | D Loss: 1.1102 | G Loss: 3.3755\n",
            "Epoch [250/500] | D Loss: 0.7679 | G Loss: 4.4525\n",
            "Epoch [300/500] | D Loss: 0.5460 | G Loss: 4.5680\n",
            "Epoch [350/500] | D Loss: 0.5910 | G Loss: 6.0243\n",
            "Epoch [400/500] | D Loss: 0.8792 | G Loss: 7.7901\n",
            "Epoch [450/500] | D Loss: 0.5640 | G Loss: 6.6575\n",
            "Epoch [500/500] | D Loss: 0.5424 | G Loss: 3.2405\n",
            "Synthetic data for class 0:\n",
            "[[-0.13512728 -0.1303159  -0.11821997 ... -0.10040642 -0.15358537\n",
            "  -0.16299437]\n",
            " [-0.2188824  -0.2050671  -0.20556138 ... -0.15817036 -0.2596087\n",
            "  -0.26030073]\n",
            " [-0.16966383 -0.15120155 -0.14558811 ... -0.12701839 -0.19798803\n",
            "  -0.19730628]\n",
            " ...\n",
            " [-0.20016119 -0.17500661 -0.17878284 ... -0.1440734  -0.22840936\n",
            "  -0.24232003]\n",
            " [-0.20182867 -0.18056881 -0.16984576 ... -0.14019625 -0.23345041\n",
            "  -0.21809308]\n",
            " [-0.17684287 -0.15460502 -0.15071091 ... -0.12145904 -0.19041179\n",
            "  -0.20026937]]\n",
            "Synthetic data for class 1:\n",
            "[[-0.14580017 -0.13319652 -0.11823285 ... -0.11022475 -0.17630734\n",
            "  -0.16129564]\n",
            " [-0.12147097 -0.12531094 -0.11544714 ... -0.09805169 -0.15118589\n",
            "  -0.15844524]\n",
            " [-0.20201121 -0.16936167 -0.14893758 ... -0.15014426 -0.22249183\n",
            "  -0.2139536 ]\n",
            " ...\n",
            " [-0.11826131 -0.1174082  -0.10077552 ... -0.08856442 -0.14160718\n",
            "  -0.14648557]\n",
            " [-0.11422379 -0.10995517 -0.10325228 ... -0.0774022  -0.11895338\n",
            "  -0.12229181]\n",
            " [-0.14595823 -0.1281833  -0.11326669 ... -0.10826848 -0.15821935\n",
            "  -0.15836495]]\n",
            "Synthetic data for class 2:\n",
            "[[-0.04495398 -0.07966812 -0.08688511 ... -0.05776787 -0.04880305\n",
            "  -0.0620113 ]\n",
            " [-0.04385783 -0.07767247 -0.07956363 ... -0.04445329 -0.03242535\n",
            "  -0.04662875]\n",
            " [-0.03433411 -0.08462286 -0.0750477  ... -0.04390102 -0.03140418\n",
            "  -0.0578942 ]\n",
            " ...\n",
            " [-0.04321091 -0.06912089 -0.07017909 ... -0.04552194 -0.04064547\n",
            "  -0.06295639]\n",
            " [-0.03570447 -0.06577246 -0.06967875 ... -0.04530183 -0.03745778\n",
            "  -0.05362302]\n",
            " [-0.03853992 -0.0753212  -0.07871961 ... -0.04204336 -0.02884099\n",
            "  -0.05623719]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "parent_dir = '/content/BONN_epilepsy_dataset/BONN epilepsy dataset'\n",
        "\n",
        "def load_bonn_data(parent_dir):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for category_dir in os.listdir(parent_dir):\n",
        "        category_path = os.path.join(parent_dir, category_dir)\n",
        "        if os.path.isdir(category_path):\n",
        "            label = len(labels)\n",
        "            for file in os.listdir(category_path):\n",
        "                file_path = os.path.join(category_path, file)\n",
        "                sample = np.loadtxt(file_path)\n",
        "                data.append(sample)\n",
        "                labels.append(label)\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "real_data, real_labels = load_bonn_data(parent_dir)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(real_data, real_labels, test_size=0.2, random_state=42)\n",
        "k = 10  #number of neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(X_train, y_train)\n",
        "print(\"Unique labels in y_train:\", np.unique(y_train))\n",
        "\n",
        "\n",
        "#print(\"Evaluation on Real Data:\")\n",
        "y_pred_real = knn.predict(X_test)\n",
        "#print(classification_report(y_test, y_pred_real))\n",
        "#print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_real))\n"
      ],
      "metadata": {
        "id": "PcwQXUqAbNKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2649397d-340e-4fd8-a23e-373400f1f14c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels in y_train: [  0 100 200]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping dictionary\n",
        "label_mapping = {0: 0, 100: 1, 200: 2}\n",
        "\n",
        "# Apply the mapping to y_train\n",
        "y_train_mapped = np.array([label_mapping[label] for label in y_train])\n",
        "\n",
        "# Verify the mapping\n",
        "print(\"Unique labels in y_train_mapped:\", np.unique(y_train_mapped))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQKO6bUFwInG",
        "outputId": "980bc72a-d7a0-44aa-ca7a-b34cb64f31aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels in y_train_mapped: [0 1 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluation on Real Data:\")\n",
        "y_pred_real = knn.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_real))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_real))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDxWwScXwN0n",
        "outputId": "cee1f2f0-b4cf-452d-e131-8567ce591bfa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Real Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        22\n",
            "         100       0.28      1.00      0.43        16\n",
            "         200       1.00      0.05      0.09        22\n",
            "\n",
            "    accuracy                           0.28        60\n",
            "   macro avg       0.43      0.35      0.17        60\n",
            "weighted avg       0.44      0.28      0.15        60\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 0 22  0]\n",
            " [ 0 16  0]\n",
            " [ 1 20  1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def generate_synthetic_data(class_label, num_samples):\n",
        "    \"\"\"\n",
        "    Generate synthetic data using the GAN.\n",
        "    Args:\n",
        "        class_label (int): The class for which data is to be generated.\n",
        "        num_samples (int): Number of samples to generate.\n",
        "    Returns:\n",
        "        numpy.ndarray: Synthetic data of shape (num_samples, num_features).\n",
        "    \"\"\"\n",
        "    generator.eval()\n",
        "    z = torch.randn(num_samples, latent_dim).to(device)\n",
        "    labels = torch.full((num_samples,), class_label, dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "        synthetic_data = generator(z, labels).detach().cpu().numpy()\n",
        "\n",
        "    return synthetic_data\n",
        "\n",
        "num_synthetic_samples = 200\n",
        "synthetic_data = []\n",
        "synthetic_labels = []\n",
        "\n",
        "#we generate synthetic data for each class\n",
        "for class_label in [0, 1, 2]:  #classes: 0 = safe 1 = boundary 2 = noise\n",
        "    data = generate_synthetic_data(class_label, num_synthetic_samples)\n",
        "    synthetic_data.append(data)\n",
        "    synthetic_labels.append(np.full(num_synthetic_samples, class_label))\n",
        "synthetic_data = np.vstack(synthetic_data)\n",
        "synthetic_labels = np.concatenate(synthetic_labels)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "#knn.fit(X_train, y_train)\n",
        "knn.fit(X_train, y_train_mapped)\n",
        "y_pred_synthetic = knn.predict(synthetic_data)\n",
        "print(\"Evaluation on Synthetic Data:\")\n",
        "print(classification_report(synthetic_labels, y_pred_synthetic))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(synthetic_labels, y_pred_synthetic))\n"
      ],
      "metadata": {
        "id": "KiREM4nacEi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d61c88-32cd-4747-a056-4bac77b9f064"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Synthetic Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       200\n",
            "           1       0.33      1.00      0.50       200\n",
            "           2       0.00      0.00      0.00       200\n",
            "\n",
            "    accuracy                           0.33       600\n",
            "   macro avg       0.11      0.33      0.17       600\n",
            "weighted avg       0.11      0.33      0.17       600\n",
            "\n",
            "Confusion Matrix:\n",
            " [[  0 200   0]\n",
            " [  0 200   0]\n",
            " [  0 200   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}
