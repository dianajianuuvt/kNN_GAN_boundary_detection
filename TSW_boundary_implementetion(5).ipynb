{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz_Bc1esGrIO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('BONN epilepsy dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/BONN_epilepsy_dataset')\n",
        "parent_dir = '/content/BONN_epilepsy_dataset'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parent_dir = '/content/BONN_epilepsy_dataset/BONN epilepsy dataset'\n",
        "print(\"Contents of the updated dataset directory:\")\n",
        "print(os.listdir(parent_dir))"
      ],
      "metadata": {
        "id": "awhqWcTaWl5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "parent_dir = '/content/BONN_epilepsy_dataset/BONN epilepsy dataset'\n",
        "directories = ['S', 'F', 'O']\n",
        "labels_map = {'S': 'boundary', 'F': 'unknown', 'O': 'safe'}  #label mapping: \"safe\" and \"boundary\"\n",
        "\n",
        "def load_data(parent_dir, directories):\n",
        "    eeg_data = []\n",
        "    labels = []\n",
        "\n",
        "    for dir_name in directories:\n",
        "        dir_path = os.path.join(parent_dir, dir_name)\n",
        "        for file_name in sorted(os.listdir(dir_path)):\n",
        "            file_path = os.path.join(dir_path, file_name)\n",
        "            with open(file_path, 'r') as file:\n",
        "                data_points = [int(line.strip()) for line in file]\n",
        "                eeg_data.append(data_points)\n",
        "                labels.append(labels_map[dir_name])\n",
        "\n",
        "    return np.array(eeg_data), np.array(labels)\n",
        "\n",
        "eeg_data, labels = load_data(parent_dir, directories)\n",
        "\n",
        "def extract_features(eeg_data):\n",
        "    \"\"\"\n",
        "    Extracts statistical features from EEG data.\n",
        "    This includes variance, kurtosis, and entropy.\n",
        "    \"\"\"\n",
        "    eeg_data_safe = np.where(eeg_data > 0, eeg_data, 1e-9)\n",
        "\n",
        "    variance = np.var(eeg_data, axis=1)\n",
        "    kurtosis = np.mean((eeg_data - np.mean(eeg_data, axis=1, keepdims=True))**4, axis=1)\n",
        "    entropy = -np.sum(eeg_data_safe * np.log2(eeg_data_safe), axis=1)\n",
        "    features = np.vstack([variance, kurtosis, entropy]).T\n",
        "    return features\n",
        "\n",
        "#kNN boundary detection for F samples\n",
        "def kNN_boundary_detection(f_data, knn_model):\n",
        "    \"\"\"\n",
        "    Classifies F samples into 'safe', 'boundary', or 'noise' based on kNN model trained on O and S.\n",
        "    \"\"\"\n",
        "    features = extract_features(f_data)\n",
        "    predictions = knn_model.predict(features)\n",
        "    return predictions\n",
        "\n",
        "#training data (only O and S samples)\n",
        "o_indices = labels == 'safe'\n",
        "s_indices = labels == 'boundary'\n",
        "\n",
        "#we combined O and S samples\n",
        "train_data = np.vstack([eeg_data[o_indices], eeg_data[s_indices]])\n",
        "train_labels = np.concatenate([labels[o_indices], labels[s_indices]])\n",
        "\n",
        "train_features = extract_features(train_data)\n",
        "\n",
        "#trainning the kNN classifier on O and S samples\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(train_features, train_labels)\n",
        "\n",
        "#F samples classification\n",
        "f_indices = labels == 'unknown'\n",
        "f_data = eeg_data[f_indices]\n",
        "eeg_types_f = kNN_boundary_detection(f_data, knn)\n",
        "\n",
        "print(\"Classifications for F (epileptic non-ictal) samples:\", eeg_types_f)\n"
      ],
      "metadata": {
        "id": "3rEujZg2Wn_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "latent_dim = 100\n",
        "data_dim = eeg_data.shape[1]\n",
        "num_classes = 3  # 0 = safe, 1 = boundary, 2 = noise\n",
        "batch_size = 64\n",
        "epochs = 500\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "class ConditionalGenerator(nn.Module):\n",
        "    def __init__(self, latent_dim, data_dim, num_classes):\n",
        "        super(ConditionalGenerator, self).__init__()\n",
        "        self.label_embedding = nn.Embedding(num_classes, num_classes)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim + num_classes, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, data_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, labels):\n",
        "        label_embeddings = self.label_embedding(labels)\n",
        "        input = torch.cat((z, label_embeddings), dim=1)\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class ConditionalDiscriminator(nn.Module):\n",
        "    def __init__(self, data_dim, num_classes):\n",
        "        super(ConditionalDiscriminator, self).__init__()\n",
        "        self.label_embedding = nn.Embedding(num_classes, num_classes)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(data_dim + num_classes, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        label_embeddings = self.label_embedding(labels)\n",
        "        input = torch.cat((x, label_embeddings), dim=1)\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "generator = ConditionalGenerator(latent_dim, data_dim, num_classes).to(device)\n",
        "discriminator = ConditionalDiscriminator(data_dim, num_classes).to(device)\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "criterion = nn.BCELoss()\n",
        "eeg_data_normalized = (eeg_data - np.min(eeg_data)) / (np.max(eeg_data) - np.min(eeg_data))\n",
        "eeg_data_normalized = 2 * eeg_data_normalized - 1\n",
        "\n",
        "\n",
        "def classify_data(eeg_data, labels, k=5):\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "    neighbors = NearestNeighbors(n_neighbors=k).fit(eeg_data)\n",
        "    types = []\n",
        "    distances, indices = neighbors.kneighbors(eeg_data)\n",
        "\n",
        "    for i, dist in enumerate(indices):\n",
        "        l = sum(labels[neighbor] == 0 for neighbor in dist)  #we count non-ictal (safe) samples\n",
        "        if k >= l > 0.5 * k:\n",
        "            types.append(0)  #safe\n",
        "        elif 0 < l <= 0.5 * k:\n",
        "            types.append(1)  #boundary\n",
        "        else:\n",
        "            types.append(2)  #noise\n",
        "    return np.array(types)\n",
        "\n",
        "real_labels = classify_data(eeg_data_normalized, labels)\n",
        "eeg_tensor = torch.tensor(eeg_data_normalized, dtype=torch.float32).to(device)\n",
        "label_tensor = torch.tensor(real_labels, dtype=torch.long).to(device)\n",
        "dataloader = DataLoader(TensorDataset(eeg_tensor, label_tensor), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for real_samples, real_class_labels in dataloader:\n",
        "        real_samples = real_samples.to(device)\n",
        "        real_class_labels = real_class_labels.to(device)\n",
        "        batch_size_curr = real_samples.size(0)\n",
        "        z = torch.randn(batch_size_curr, latent_dim).to(device)\n",
        "        fake_class_labels = torch.randint(0, num_classes, (batch_size_curr,), dtype=torch.long).to(device)\n",
        "        fake_samples = generator(z, fake_class_labels).detach()\n",
        "        real_targets = torch.ones(batch_size_curr, 1).to(device)\n",
        "        fake_targets = torch.zeros(batch_size_curr, 1).to(device)\n",
        "        optimizer_D.zero_grad()\n",
        "        real_loss = criterion(discriminator(real_samples, real_class_labels), real_targets)\n",
        "        fake_loss = criterion(discriminator(fake_samples, fake_class_labels), fake_targets)\n",
        "        d_loss = real_loss + fake_loss\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "        z = torch.randn(batch_size_curr, latent_dim).to(device)\n",
        "        target_class_labels = torch.randint(0, num_classes, (batch_size_curr,), dtype=torch.long).to(device)\n",
        "        fake_samples = generator(z, target_class_labels)\n",
        "        optimizer_G.zero_grad()\n",
        "        g_loss = criterion(discriminator(fake_samples, target_class_labels), real_targets)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "for label in range(num_classes):\n",
        "    z = torch.randn(1000, latent_dim).to(device)\n",
        "    labels = torch.full((1000,), label, dtype=torch.long).to(device)\n",
        "    synthetic_data = generator(z, labels).detach().cpu().numpy()\n",
        "    print(f\"Synthetic data for class {label}:\")\n",
        "    print(synthetic_data)\n"
      ],
      "metadata": {
        "id": "6txnbUMhWqFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "parent_dir = '/content/BONN_epilepsy_dataset/BONN epilepsy dataset'\n",
        "\n",
        "def load_bonn_data(parent_dir):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for category_dir in os.listdir(parent_dir):\n",
        "        category_path = os.path.join(parent_dir, category_dir)\n",
        "        if os.path.isdir(category_path):\n",
        "            label = len(labels)\n",
        "            for file in os.listdir(category_path):\n",
        "                file_path = os.path.join(category_path, file)\n",
        "                sample = np.loadtxt(file_path)\n",
        "                data.append(sample)\n",
        "                labels.append(label)\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "real_data, real_labels = load_bonn_data(parent_dir)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(real_data, real_labels, test_size=0.2, random_state=42)\n",
        "k = 10  #number of neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(X_train, y_train)\n",
        "print(\"Unique labels in y_train:\", np.unique(y_train))\n",
        "\n",
        "\n",
        "#print(\"Evaluation on Real Data:\")\n",
        "y_pred_real = knn.predict(X_test)\n",
        "#print(classification_report(y_test, y_pred_real))\n",
        "#print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_real))\n"
      ],
      "metadata": {
        "id": "PcwQXUqAbNKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping dictionary\n",
        "label_mapping = {0: 0, 100: 1, 200: 2}\n",
        "\n",
        "# Apply the mapping to y_train\n",
        "y_train_mapped = np.array([label_mapping[label] for label in y_train])\n",
        "\n",
        "# Verify the mapping\n",
        "print(\"Unique labels in y_train_mapped:\", np.unique(y_train_mapped))\n"
      ],
      "metadata": {
        "id": "GQKO6bUFwInG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluation on Real Data:\")\n",
        "y_pred_real = knn.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_real))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_real))"
      ],
      "metadata": {
        "id": "TDxWwScXwN0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def generate_synthetic_data(class_label, num_samples):\n",
        "    \"\"\"\n",
        "    Generate synthetic data using the GAN.\n",
        "    Args:\n",
        "        class_label (int): The class for which data is to be generated.\n",
        "        num_samples (int): Number of samples to generate.\n",
        "    Returns:\n",
        "        numpy.ndarray: Synthetic data of shape (num_samples, num_features).\n",
        "    \"\"\"\n",
        "    generator.eval()\n",
        "    z = torch.randn(num_samples, latent_dim).to(device)\n",
        "    labels = torch.full((num_samples,), class_label, dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "        synthetic_data = generator(z, labels).detach().cpu().numpy()\n",
        "\n",
        "    return synthetic_data\n",
        "\n",
        "num_synthetic_samples = 200\n",
        "synthetic_data = []\n",
        "synthetic_labels = []\n",
        "\n",
        "#we generate synthetic data for each class\n",
        "for class_label in [0, 1, 2]:  #classes: 0 = safe 1 = boundary 2 = noise\n",
        "    data = generate_synthetic_data(class_label, num_synthetic_samples)\n",
        "    synthetic_data.append(data)\n",
        "    synthetic_labels.append(np.full(num_synthetic_samples, class_label))\n",
        "synthetic_data = np.vstack(synthetic_data)\n",
        "synthetic_labels = np.concatenate(synthetic_labels)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "#knn.fit(X_train, y_train)\n",
        "knn.fit(X_train, y_train_mapped)\n",
        "y_pred_synthetic = knn.predict(synthetic_data)\n",
        "print(\"Evaluation on Synthetic Data:\")\n",
        "print(classification_report(synthetic_labels, y_pred_synthetic))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(synthetic_labels, y_pred_synthetic))\n"
      ],
      "metadata": {
        "id": "KiREM4nacEi0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}